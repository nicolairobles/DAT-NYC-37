{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Model Evaluation\n",
    "First, credits: many of the examples used come from the brilliant [Peter Prettenhofer](https://twitter.com/pprett) of [DataRobot](http://www.datarobot.com/) and Rob Hall's DAT13 class: https://github.com/hallr/DAT13<br><br>\n",
    "\n",
    "\n",
    "Third, let's take a minute to discuss [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html), [`make_pipeline`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) and [`Polynomial Features`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Intro: Splitting our dataset into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'X': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'y': [11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "})\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(df['X'], df['y'], test_size=0.30)\n",
    "\n",
    "print \"Full dataset:\"\n",
    "print \"X = \", df['X'].values\n",
    "print \"y = \", df['y'].values, \"\\n\"\n",
    "\n",
    "print \"Training (\\\"In-sample\\\") data:\"\n",
    "print \"X_train = \", X_train.values\n",
    "print \"y_train = \", y_train.values, \"\\n\"\n",
    "\n",
    "print \"Test (\\\"Out-of-sample\\\") data:\"\n",
    "print \"X_test = \", X_test.values\n",
    "print \"y_test = \", y_test.values\n",
    "\n",
    "# Side note: We can also do the same thing in pandas with df.sample:\n",
    "\n",
    "# df.sample(frac = .7, random_state = 0).sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** We need to \"hold out\" some fraction of our dataset (30% in the above example) to test the results of our model. Above, we held-out 70% of our dataset to fit the model. Once fit, we'll evaluate our model against the test data.\n",
    "\n",
    "*Test data should be selected randomly to avoid bias*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Model Complexity and the problem of overfitting\n",
    "\n",
    "[Polynomial regression](http://en.wikipedia.org/wiki/Polynomial_regression) fits a n-th order polynomial to our data using least squares. \n",
    "\n",
    "[Linear regression](http://en.wikipedia.org/wiki/Linear_regression) is a special case of polynomial regression which fits a polynomial of degree=1.\n",
    "<br>\n",
    "\n",
    "To apply Polynomial regression, we add additional features for each of the higher-order polynomial terms. To illustrate:\n",
    "\n",
    "Suppose we have a dataset with features $x_1$, $x_2$ and target variable $y$. We could apply polynomial regression for a 2nd order polynomial by adding additional features $x_1^2, x_2^2, x_1 x_2$\n",
    "\n",
    "In other words, in the linear case we are fitting:\n",
    "\n",
    "$y = \\boldsymbol\\beta_1 x_1 + \\boldsymbol\\beta_2 x_2$\n",
    "\n",
    "And we're just adding additional features for the polynomial terms to our model:\n",
    "\n",
    "$y = \\boldsymbol\\beta_1 x_1 + \\boldsymbol\\beta_2 x_2 + \\boldsymbol\\beta_3 x_1^2 + \\boldsymbol\\beta_4 x_2^2 + \\boldsymbol\\beta_5 x_1 x_2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "figsize(5,5)\n",
    "\n",
    "np.random.seed(9)\n",
    "\n",
    "# A simple sinusoidal function\n",
    "def f(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "# generate points along x used to plot\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "n_samples = 100\n",
    "\n",
    "# In your own words, what do you think np.random.uniform does?\n",
    "X = np.random.uniform(0, 1, size=n_samples)[:, np.newaxis]\n",
    "\n",
    "y = f(X) + np.random.normal(scale=0.3, size=n_samples)[:, np.newaxis]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)\n",
    "\n",
    "fig, axes = plt.subplots(1,1)\n",
    "\n",
    "axes.plot(x_plot, f(x_plot), label='ground truth: sin(2pi*x)', color='green')\n",
    "axes.scatter(X_train, y_train, label='training data', s=100)\n",
    "axes.set_ylim((-2, 2))\n",
    "axes.set_xlim((0, 1))\n",
    "axes.set_ylabel('y')\n",
    "axes.set_xlabel('x')\n",
    "\n",
    "axes.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_approximation(est, ax, label=None):\n",
    "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n",
    "    ax.plot(x_plot, f(x_plot), label='ground truth', color='green')\n",
    "    ax.scatter(X_train, y_train, s=100)\n",
    "    ax.plot(x_plot, est.predict(x_plot[:, np.newaxis]), color='red', label=label)\n",
    "    ax.set_ylim((-2, 2))\n",
    "    ax.set_xlim((0, 1))\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.legend(loc='upper right', frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "degree = 1\n",
    "\n",
    "# Step 1: Transform my input into a Polynomial feature space:\n",
    "\n",
    "# Example Input\n",
    "#     x = [1, 3, 4, 2, 3]\n",
    "#\n",
    "# x_poly_features = PolynomialFeatures(degree=2)\n",
    "# x_poly_features.fit_transform(x)\n",
    "#\n",
    "# Example output:\n",
    "# array([[ 1,  1,  1],\n",
    "#        [ 1,  3,  9],\n",
    "#        [ 1,  4, 16],\n",
    "#        [ 1,  2,  4],\n",
    "#        [ 1,  3,  9]])\n",
    "\n",
    "# Step 2: Apply Linear Regression\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "est.fit(X_train, y_train)\n",
    "plot_approximation(est, ax, label='degree=%d' % degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Qualitatively, how would you characterize this fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn:\n",
    "Plot the fit of a polynomial of degree 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "degree = 2\n",
    "\n",
    "# Step 1: Transform my input into a Polynomial feature space:\n",
    "\n",
    "# Example Input\n",
    "#     x = [1, 3, 4, 2, 3]\n",
    "#\n",
    "# x_poly_features = PolynomialFeatures(degree=2)\n",
    "# x_poly_features.fit_transform(x)\n",
    "#\n",
    "# Example output:\n",
    "# array([[ 1,  1,  1],\n",
    "#        [ 1,  3,  9],\n",
    "#        [ 1,  4, 16],\n",
    "#        [ 1,  2,  4],\n",
    "#        [ 1,  3,  9]])\n",
    "\n",
    "# Step 2: Apply Linear Regression\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "est.fit(X_train, y_train)\n",
    "plot_approximation(est, ax, label='degree=%d' % degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fit of a polynomial of degree 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "degree = 3\n",
    "\n",
    "# Step 2: Apply Linear Regression\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "est.fit(X_train, y_train)\n",
    "plot_approximation(est, ax, label='degree=%d' % degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fit of a polynomial of degree 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "degree = 9\n",
    "\n",
    "# Step 2: Apply Linear Regression\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "est.fit(X_train, y_train)\n",
    "plot_approximation(est, ax, label='degree=%d' % degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What happens as we increase the degree of polynomial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Which polynomial should we choose? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "degrees=10\n",
    "\n",
    "train_error = np.empty(degrees)\n",
    "test_error = np.empty(degrees)\n",
    "for degree in range(degrees):\n",
    "    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    est.fit(X_train, y_train)\n",
    "    train_error[degree] = mean_squared_error(y_train, est.predict(X_train))\n",
    "    test_error[degree] = mean_squared_error(y_test, est.predict(X_test))\n",
    "\n",
    "plt.plot(np.arange(degrees), train_error, color='green', label='train')\n",
    "plt.plot(np.arange(degrees), test_error, color='red', label='test')\n",
    "plt.ylim((0.0, 1e0))\n",
    "plt.ylabel('log(mean squared error)')\n",
    "plt.xlabel('degree')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the degree of the polynomial (our proxy for model complexity), the lower the training error. The testing error decreases too, but it eventually reaches its minimum at a degree of three and then starts increasing at a degree of seven. \n",
    "\n",
    "This phenomenon is called *overfitting*: the model is already so complex that it fits the idiosyncrasies of our training data, idiosyncrasies which limit the model's ability to generalize (as measured by the testing error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the optimal choice for the degree of the polynomial approximation would be between three and six. So when we get some data, we could fit a bunch of polynomials and then choose the one that minimizes MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hand picking polynomials is hard work, and data scientists are lazy so....\n",
    "...we would like a method that eliminates the need to manually select the degree of the polynomial: we can add a constraint to our linear regression model that constrains the magnitude of the coefficients in the regression model. This constraint is called the regularization term and the technique is often called shrinkage in the statistical community because it shrinks the coefficients towards zero. In the context of polynomial regression, constraining the magnitude of the regression coefficients effectively is a smoothness assumption: by constraining the L2 norm of the regression coefficients we express our preference for smooth functions rather than wiggly functions.\n",
    "\n",
    "A popular regularized linear regression model is Ridge Regression. This adds the L2 norm of the coefficients to the ordinary least squares objective:\n",
    "\n",
    "  $J(\\boldsymbol\\beta) = \\frac{1}{n}\\sum_{i=0}^n (y_i - \\boldsymbol\\beta^T \\mathbf{x}_i')^2 + \\alpha \\|\\boldsymbol\\beta\\|_2$\n",
    "\n",
    "where $\\boldsymbol\\beta$ is the vector of coefficients including the intercept term and $\\mathbf{x}_i'$ is the i-th feature fector including a dummy feature for the intercept. The L2 norm term is weighted by a regularization parameter ``alpha``: if ``alpha=0`` then you recover the Ordinary Least Squares regression model. The larger the value of ``alpha`` the higher the smoothness constraint.\n",
    "\n",
    "Below you can see the approximation of a [``sklearn.linear_model.Ridge``](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) estimator fitting a polynomial of degree nine for various values of ``alpha`` (left) and the corresponding coefficient loadings (right). The smaller the value of ``alpha`` the higher the magnitude of the coefficients, so the functions we can model can be more and more wiggly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax_rows = plt.subplots(4, 2, figsize=(15, 20))\n",
    "\n",
    "def plot_coefficients(est, ax, label=None, yscale='log'):\n",
    "    coef = est.steps[-1][1].coef_.ravel()\n",
    "    if yscale == 'log':\n",
    "        ax.semilogy(np.abs(coef), marker='o', label=label)\n",
    "        ax.set_ylim((1e-1, 1e8))\n",
    "    else:\n",
    "        ax.plot(np.abs(coef), marker='o', label=label)\n",
    "    ax.set_ylabel('abs(coefficient)')\n",
    "    ax.set_xlabel('coefficients')\n",
    "    ax.set_xlim((1, 9))\n",
    "\n",
    "degree = 9\n",
    "alphas = [0.0, 1e-8, 1e-5, 1e-1]\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    est = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\n",
    "    est.fit(X_train, y_train)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='Ridge(alpha=%r) coefficients' % alpha)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Regularization techniques\n",
    "\n",
    "In the above example we used Ridge Regression, a regularized linear regression technique that puts an [L2 norm](http://mathworld.wolfram.com/L2-Norm.html) penalty on the regression coefficients. Another popular regularization technique is the LASSO, a technique which puts an [L1 norm](http://mathworld.wolfram.com/L1-Norm.html) penalty instead. The difference between the two is that the LASSO leads to sparse solutions, driving most coefficients to zero, whereas Ridge Regression leads to dense solutions, in which most coefficients are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "fig, ax_rows = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "degree = 9\n",
    "alphas = [1e-3, 1e-2]\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    est = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=alpha))\n",
    "    est.fit(X_train, y_train)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='Lasso(alpha=%r) coefficients' % alpha, yscale=None)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Exercise (Optional, but recommended)\n",
    "\n",
    "A criminologist studying the relationship between income level and assults in U.S. cities (among other things) collected the following data for 2215 communities. The dataset can be found in the [UCI machine learning site](http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized).\n",
    "\n",
    "We are interested in the per capita assult rate and its relation to median income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crime = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00211/CommViolPredUnnormalizedData.txt\", \n",
    "                    header = None, na_values  = '?',\n",
    "                    names = ['communityname', 'state', 'countyCode', 'communityCode', 'fold', 'population', 'householdsize', \n",
    "                             'racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29', \n",
    "                             'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome', 'pctWWage', 'pctWFarmSelf', \n",
    "                             'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire', 'medFamInc', 'perCapInc', 'whitePerCap', \n",
    "                             'blackPerCap', 'indianPerCap', 'AsianPerCap', 'OtherPerCap', 'HispPerCap', 'NumUnderPov', \n",
    "                             'PctPopUnderPov', 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctEmploy', \n",
    "                             'PctEmplManu', 'PctEmplProfServ', 'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', \n",
    "                             'MalePctNevMarr', 'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par', 'PctKids2Par', \n",
    "                             'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids', 'PctWorkMom', 'NumKidsBornNeverMar', \n",
    "                             'PctKidsBornNeverMar', 'NumImmig', 'PctImmigRecent', 'PctImmigRec5', 'PctImmigRec8', \n",
    "                             'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5', 'PctRecImmig8', 'PctRecImmig10', \n",
    "                             'PctSpeakEnglOnly', 'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup', \n",
    "                             'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous', 'PctPersOwnOccup', \n",
    "                             'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR', 'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', \n",
    "                             'PctVacantBoarded', 'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb', \n",
    "                             'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart', 'OwnOccQrange', 'RentLowQ', 'RentMedian', \n",
    "                             'RentHighQ', 'RentQrange', 'MedRent', 'MedRentPctHousInc', 'MedOwnCostPctInc', \n",
    "                             'MedOwnCostPctIncNoMtg', 'NumInShelters', 'NumStreet', 'PctForeignBorn', 'PctBornSameState', \n",
    "                             'PctSameHouse85', 'PctSameCity85', 'PctSameState85', 'LemasSwornFT', 'LemasSwFTPerPop', \n",
    "                             'LemasSwFTFieldOps', 'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop', \n",
    "                             'PolicReqPerOffic', 'PolicPerPop', 'RacialMatchCommPol', 'PctPolicWhite', 'PctPolicBlack', \n",
    "                             'PctPolicHisp', 'PctPolicAsian', 'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz', \n",
    "                             'PolicAveOTWorked', 'LandArea', 'PopDens', 'PctUsePubTrans', 'PolicCars', 'PolicOperBudg', \n",
    "                             'LemasPctPolicOnPatr', 'LemasGangUnitDeploy', 'LemasPctOfficDrugUn', 'PolicBudgPerPop', \n",
    "                             'murders', 'murdPerPop', 'rapes', 'rapesPerPop', 'robberies', 'robbbPerPop', 'assaults', \n",
    "                             'assaultPerPop', 'burglaries', 'burglPerPop', 'larcenies', 'larcPerPop', 'autoTheft', \n",
    "                             'autoTheftPerPop', 'arsons', 'arsonsPerPop', 'ViolentCrimesPerPop', 'nonViolPerPop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit a simple linear regression model to the data with `np.log(crime.assaults)` as the dependent variable and `np.log(crime.medIncome)` as the independent variable. Plot the estimated regression line.\n",
    "\n",
    "4. Test whether there is a linear relationship between `assaults` and `medIncome` at level $\\alpha=0.05$. State the null hypothesis, the alternative, the conclusion and the $p$-value.\n",
    "\n",
    "5. Give a 95% confidence interval for the slope of the regression line. Interpret your interval.\n",
    "\n",
    "6. Report the $R^2$ and the adjusted $R^2$ of the model, as well as an estimate of the variance of the errors in the model.\n",
    "\n",
    "7. Go to [archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized](http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized) and pick out a few other factors that might help you predict `assults`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
