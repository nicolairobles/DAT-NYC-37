{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Mining: Vectorization, TF-IDF, & Cosine Similarity\n",
    "=====\n",
    "\n",
    "1. The Vector Space Model\n",
    "====\n",
    "\n",
    "2. Vector Normalization\n",
    "====\n",
    "\n",
    "3. Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "====\n",
    "\n",
    "4. Cosine Similarity\n",
    "====\n",
    "***\n",
    "\n",
    "####This Notebook is based upon the tutorials found at Pyevolve, by Christian S. Perone:\n",
    "\n",
    "####http://blog.christianperone.com/?p=1589"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1. The Vector Space Model (VSM)\n",
    "=====\n",
    "\n",
    "- In information retrieval or text mining, the 'term frequency – inverse document frequency' also called tf-idf, is a well known method to evaluate how important a word is within a corpus of documents. \n",
    "\n",
    "- Tf-idf is also a very interesting way to convert the textual representation of information into a Vector Space Model (VSM), or into sparse features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vector Space\n",
    "\n",
    "- The first step in creating a representation of a document in vector space is to create a dictionary of terms\n",
    "\n",
    "- To do this, you simply select all terms from the document, estimate their frequencies and use these frequencies as numerical vectors\n",
    "\n",
    "- Stop words will be removed\n",
    "\n",
    "####An extremely basic document space:\n",
    "\n",
    "#####Training Document Set:\n",
    "\n",
    "d1: The sky is blue.\n",
    "\n",
    "d2: The sun is bright.\n",
    "\n",
    "#####Test Document Set:\n",
    "\n",
    "d3: The sun in  the sky is bright.\n",
    "\n",
    "d4: We can see the shining sun, the bright sun.\n",
    "\n",
    "- Create a dictionary of words, otherwise known as an index vocabulary\n",
    "\n",
    "- Using the documents d1 and d2 from the training document set, we’ll have the following index vocabulary, denoted as $E(t)$, where the $t$ is a word:\n",
    "\n",
    "$E(t) = \\left\\{\\begin{matrix} \n",
    "1, & \\text{if }t\\text{ is } \"blue\" \\\\\n",
    "2, & \\text{if }t\\text{ is } \"sun\" \\\\\n",
    "3, & \\text{if }t\\text{ is } \"bright\" \\\\\n",
    "4, & \\text{if }t\\text{ is } \"sky\"\\end{matrix}\\right.$\n",
    "\n",
    "- $E(t)$ converts a word or term to an integer\n",
    "\n",
    "- Converting the training documents is a Python \"fit\"\n",
    "\n",
    "- Using the index vocabulary we can convert the test document set into a vector space, which is a Python \"transform\"\n",
    "\n",
    "- For example, the first vector value will represent frequency of the word 'blue'\n",
    "\n",
    "- Term Frequency (requiring $2$ arguments - a term, $t$, and a document $d$) is defined as:\n",
    "\n",
    "$tf(t, d) = \\sum_{x \\in d}fr(x, t)$\n",
    "\n",
    "where $fr(x, t) = \\left\\{\\begin{matrix} 1, & \\text{if }t = x \\\\ 0, & \\text{otherwise} \\end{matrix}\\right.$\n",
    "\n",
    "- $x$ is all the terms or words in the document\n",
    "\n",
    "- it is just a frequency counter\n",
    "\n",
    "- $tf(t,d)$ returns how many times the term t is present in the document d. \n",
    "\n",
    "- For example: $tf(\\text{'sun'}, d4) = 2$, since we have only two occurrences of the term “sun” in the document d4.\n",
    "\n",
    "####The Document Vector\n",
    "\n",
    "- A Document Vector is defined as:\n",
    "\n",
    "$\\overrightarrow{v_{d_{n}}} = (tf(t_{1}, d_{n}), tf(t_{2}, d_{n}), tf(t_{3},d_{n}),...,tf(t_{n}, d_{n}))$\n",
    "\n",
    "- each dimension of the document vector represents the term frequency of a word in the index vocabularly from the document\n",
    "\n",
    "- For example: the $tf(t_1,d_2)$ represents the frequency-term of term $1$ or $t_1$ ('blue' in our example index vocabulary) in the document $d_2$.\n",
    "\n",
    "####The Transform:\n",
    "\n",
    "- Representing documents $d_3$ and $d_4$ (the test documents) as vectors:\n",
    "\n",
    "$\\overrightarrow{v_{d_{3}}} = (tf(t_{1}, d_{3}), tf(t_{2}, d_{3}), tf(t_{3},d_{3}),...,tf(t_{n}, d_{3}))$\n",
    "\n",
    "$\\overrightarrow{v_{d_{4}}} = (tf(t_{1}, d_{4}), tf(t_{2}, d_{4}), tf(t_{3},d_{4}),...,tf(t_{n}, d_{3}))$\n",
    "\n",
    "$\\overrightarrow{v_{d_{3}}} = (tf(blue, d_{3}), tf(sun, d_{3}), tf(bright,d_{3}), tf(sky, d_{3}))$\n",
    "\n",
    "$\\overrightarrow{v_{d_{4}}} = (tf(blue, d_{4}), tf(sun, d_{4}), tf(bright,d_{4}), tf(sky, d_{4}))$\n",
    "\n",
    "#####Which evaluates to:\n",
    "\n",
    "$\\overrightarrow{v_{d_{3}}} = (0,1,1,1)$\n",
    "\n",
    "$\\overrightarrow{v_{d_{4}}} = (0,2,1,0)$\n",
    "\n",
    "- recall documents $d_3$ and $d_4$ are:\n",
    "\n",
    "d3: The sun in the sky is bright.\n",
    "\n",
    "d4: We can see the shining sun, the bright sun.\n",
    "\n",
    "- The resulting vector $v_{d_3}$ shows that we have, in order, 0 occurrences of the term “blue”, 1 occurrence of the term “sun”, and so on. \n",
    "\n",
    "- In vector $v_{d_3}$, we have 0 occurences of the term “blue”, 2 occurrences of the term “sun”, etc.\n",
    "\n",
    "- each document is now represented as a vector\n",
    "\n",
    "- the corpus of documents can now be represented by a matrix containing the vectors\n",
    "\n",
    "- the matrix will have shape $D \\times F$, where |D| is the cardinality of the document space, or how many documents there are, and the F is the number of features, in our case represented by the index vocabulary size\n",
    "\n",
    "- an example of the matrix representation of the vectors described above is:\n",
    "\n",
    "$M_{|D| \\times F} = \\begin{bmatrix} 0 & 1 & 1 & 1 \\\\ 0 & 2 & 1 & 0 \\end{bmatrix} $\n",
    "\n",
    "- such matrices, representing the term frequencies, tend to be very sparse (with majority of terms zeroed), and that’s why they are commonly represented as sparse matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Now, in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "print vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = (\"The sky is blue.\",\n",
    "             \"The sun is bright.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_set = (\"The sun in the sky is bright.\",\n",
    "            \"We can see the shining sun, the bright sun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'blue': 0, u'bright': 1, u'sun': 4, u'is': 2, u'sky': 3, u'the': 5}\n"
     ]
    }
   ],
   "source": [
    "vectorizer.fit_transform(train_set)\n",
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####*Q: What do you notice about the index vocabulary?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#what parameter do we need to set when instantiating the CountVectorizer object instance?\n",
    "vectorizer = CountVectorizer(stop_words = \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'blue': 0, u'sun': 3, u'bright': 1, u'sky': 2}\n"
     ]
    }
   ],
   "source": [
    "count_vectors = vectorizer.fit_transform(train_set)\n",
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Now that we have constructed our vocabulary with the fit function, let's pass our test set through that vocabulary using the transform function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Remember its vectorizer.transform (not fit!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "sparse_matrix = vectorizer.transform(test_set)\n",
    "print sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Note that the sparse matrix created, called \"sparse_matrix\" is a Scipy sparse matrix, with elements stored in a Coordinate format. But we can convert it into a dense format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 1, 1],\n",
       "        [0, 1, 0, 2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Each row in the above matrix represents a document in our test set ($d3$ and $d4$). \n",
    "\n",
    "####So, this gives us a (very basic) matrix of term frequencies in our test documents. \n",
    "\n",
    "####We have mapped our text documents into a vector space!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Vector Normalization\n",
    "=====\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the main problem with the term-frequency approach is that it scales up frequent terms and scales down rare terms, which are empirically more informative than the high frequency terms. \n",
    "\n",
    "- The basic intuition is that a term that occurs frequently in many documents is not a good discriminator. \n",
    "\n",
    "- The important question, if the aim is to classify documents, is why you would emphasize a term which is almost present in the entire corpus of all your documents?\n",
    "\n",
    "####Term Frequency minus Inverse Document Frequency (tf-idf) solves this problem\n",
    "\n",
    "- tf-idf provides an indicator of the importance of a word found within a document across the entire corpus of documents \n",
    "\n",
    "- tf-idf incorporates local and global parameters; it takes into consideration not only the local term frequency but also the term frequency within the document collection (global). \n",
    "\n",
    "- tf-idf basically weights the frequent terms less than than the rarer terms using a logarithmic scale, because a term that occurs 10 times more than another isn’t 10 times more important \n",
    "\n",
    "#####Normalized Term Frequency\n",
    "\n",
    "- the use of simple term frequency could lead us to problems like *keyword spamming*\n",
    "\n",
    "- keyword spamming occurs when we have a repeated term in a document whose purpose is to improve the document's ranking on an Information Retrieval (IR) system\n",
    "\n",
    "- term repeating can also create a bias towards long documents, making them look more important than they are just because of the high frequency of a term in the document\n",
    "\n",
    "- to overcome this problem the vector space of a document is usually normalized \n",
    "\n",
    "#####Normalizing the Document Vectors\n",
    "\n",
    "$\\overrightarrow{v_{d_{4}}} = (0, 2, 1, 0)$\n",
    "\n",
    "- normalizing a vector means making it's length $1$\n",
    "\n",
    "- a normalized vector is denoted using the 'hat' notation:\n",
    "\n",
    "$\\hat{v}$\n",
    "\n",
    "- The definition of $\\hat{v}$ is:\n",
    "\n",
    "$\\hat{v} = \\frac{\\overrightarrow{v}}{\\|\\overrightarrow{v}\\|_{p}}$\n",
    "\n",
    "- $\\overrightarrow{v}$ is the vector going to be normalized and the $\\|\\overrightarrow{v}\\|_{p}$ is the norm (magnitude, length) of the vector $\\overrightarrow{v}$ in the $L^{p}$ space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Diagrammatically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Assets/10.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Calculation of 'length' and Lebesque Spaces\n",
    "\n",
    "- to understand this, you must understand the motivation of the $L^{p}$ spaces, also called Lebesgue spaces\n",
    "\n",
    "- we have already seen examples of Lebesgue spaces in terms of regression and classification coefficient penalties!!\n",
    "\n",
    "- L1 and L2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Lebesgue Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Assets/11.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####The L2 Norm:\n",
    "\n",
    "$\\|\\overrightarrow{v}\\|=\\sqrt{v^{2}_{1} + v^{2}_{2} + v^{2}_{3} + ... + v^{2}_{n}}$\n",
    "\n",
    "- this is *NOT* the only way to define length\n",
    "\n",
    "- that’s why you see (sometimes) a number $p$ together with the norm notation, like in $|v|^{p}$. \n",
    "\n",
    "- that’s because it could be generalized as:\n",
    "\n",
    "$\\|\\overrightarrow{v}\\|_{p} = \\left(|v_{1}|^{p} + |v_{2}|^{p} + |v_{3}|^{p} + ... + |v_{n}|^{p}\\right)^{\\frac{1}{p}}$\n",
    "\n",
    "- when you see L2-norm, it means Euclidean norm, or a norm with p=2 (this is the most common norm used)\n",
    "\n",
    "- if you see an unqualified length measure (without the $p$ number), you assume L2.\n",
    "\n",
    "- L1-norm, $p=1$, is defined as:\n",
    "\n",
    "$\\|\\overrightarrow{v}\\|_{1} = \\left(|v_{1}| + |v_{2}| + |v_{3}| + ... + |v_{n}|\\right)$\n",
    "\n",
    "- which is nothing more than a simple sum of the components of the vector, also known as Taxicab distance, also known as Manhattan distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Assets/manhattan_distance.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Taxicab geometry versus Euclidean distance\n",
    "\n",
    "- using Taxicab distance the blue, red and yellow lines pictured have the same length (12) for the same route. \n",
    "\n",
    "- Using L2-norma, the green line has length $\\sqrt{36 + 36} = 8.48$.\n",
    "\n",
    "- Note that you can also use any norm to normalize a vector.\n",
    "\n",
    "- Let's use L2, which is also the default in sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Normalizing the Vectors\n",
    "\n",
    "$\\hat{v} = \\frac{\\overrightarrow{v}}{\\| \\overrightarrow{v} \\|_{p}}$\n",
    "\n",
    "$\\hat{v_{d_{4}}} = \\frac{\\overrightarrow{v_{d_{4}}}}{\\| \\overrightarrow{v_{d_{4}}} \\|_{2}}$\n",
    "\n",
    "$\\hat{v_{d_{4}}} = \\frac{(0,2,1,0)}{\\sqrt{0^{2} + 2^{2} + 1^{2} + 0^{2}}}$\n",
    "\n",
    "$\\hat{v_{d_{4}}} = \\frac{(0,2,1,0)}{\\sqrt{5}}$\n",
    "\n",
    "$\\hat{v_{d_{4}}} = (0.0, 0.89, 0.45, 0.0)$\n",
    "\n",
    "- $\\hat{v_{d_{4}}}$ now has length 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 The Term Frequency - Inverse Document Frequency (TF-IDF) Weight\n",
    "=====\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Training Document Set:\n",
    "\n",
    "d1: The sky is blue\n",
    "\n",
    "d2: The sun is bright\n",
    "\n",
    "#####Test Document Set:\n",
    "\n",
    "d3: The sun in the sky is bright\n",
    "\n",
    "d4: We can see the shining sun, the bright sun\n",
    "\n",
    "\n",
    "####With terms:\n",
    "\n",
    "blue, sun, bright, sky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the document space can be defined then as $D = {d1, d2, ..., dn}$ where $n$ is the number of documents in our corpus, and in our case as $D_{train} = {d1, d2}$ and $D_{test} = {d3, d4}$\n",
    "\n",
    "- the cardinality of our document space is defined by $|D_{train}| = 2$ and $|D_{test}| = 2$, since we have only 2 two documents for training and testing\n",
    "\n",
    "- the training and test sets do not need to have the same cardinality\n",
    "\n",
    "#####idf (inverse document frequency) is defined:\n",
    "\n",
    "$idf(t) = log\\frac{|D|}{1+|\\{d:t \\in d\\}|}$\n",
    "\n",
    "- where $|\\{d : t \\in d\\}|$ is the number of documents where the term t appears\n",
    "\n",
    "- add 1 into the formula to avoid zero-division.\n",
    "\n",
    "#####The formula for the tf-idf is then:\n",
    "\n",
    "$tf-idf(t) = tf(t,d) \\times idf(t)$\n",
    "\n",
    "#####*This formula has an important consequence: a high weight of the tf-idf calculation is reached when we have a high term frequency (tf) in the given document (local parameter) and a low document frequency of the term in the whole collection (global parameter).*\n",
    "\n",
    "- let’s calculate the idf for each feature present in the feature matrix with the term frequency we have calculated in the first section above: \n",
    "\n",
    "$M_{train} = \\begin{bmatrix} 0 & 1 & 1 & 1 \\\\ 0 & 2 & 1 & 0 \\end{bmatrix}$\n",
    "\n",
    "- since we have 4 features, we have to calculate $idf(t1)$, $idf(t2)$, $idf(t3)$, $idf(t4)$:\n",
    "\n",
    "$idf(t_{1}) = log\\frac{|D|}{1+|\\{d:t\\in d\\}|} = ln\\frac{2}{1} = 0.6931$\n",
    "\n",
    "$idf(t_{2}) = log\\frac{|D|}{1+|\\{d:t\\in d\\}|} = ln\\frac{2}{3} = -0.4055$\n",
    "\n",
    "$idf(t_{3}) = log\\frac{|D|}{1+|\\{d:t\\in d\\}|} = ln\\frac{2}{3} = -0.4055$\n",
    "\n",
    "$idf(t_{4}) = log\\frac{|D|}{1+|\\{d:t\\in d\\}|} = ln\\frac{2}{2} = 0.000$\n",
    "\n",
    "- these idf weights can be represented by a vector as:\n",
    "\n",
    "$\\overrightarrow{idf_{train}} = (0.6931, -0.4055, -0.4055, 0.0000)$\n",
    "\n",
    "- now that we have our matrix with the term frequency $M_{train}$ and the vector representing the idf for each feature of our matrix $idf_{train}$, we can calculate our tf-idf weights. \n",
    "\n",
    "- what we have to do is a simple multiplication of each column of the matrix $M_{train}$ with the respective $idf_{train}$ vector dimension. \n",
    "\n",
    "- To do that, we can create a square diagonal matrix called $M_{idf}$ with both the vertical and horizontal dimensions equal to the vector $idf_{train}$ dimension:\n",
    "\n",
    "$M_{idf} = \\begin{bmatrix} 0.6931 & 0 & 0 & 0 \\\\ 0 & -0.4055 & 0 & 0 \\\\ 0 & 0 & -0.4055 & 0 \\\\ 0 & 0 & 0 & 0.000 \\end{bmatrix}$\n",
    "\n",
    "- and then multiply it by the term frequency matrix, so the final result can be defined then as:\n",
    "\n",
    "$M_{tf-idf} = M_{train} \\times M_{idf}$\n",
    "\n",
    "#####Matrix multiplication isn’t commutative, the result of A x B will be different than the result of the B x A, and this is why the $M_{idf}$ is on the right side of the multiplication, to accomplish the desired effect of multiplying each idf value to its corresponding feature:\n",
    "\n",
    "$\\begin{bmatrix} tf(t_{1}, d_{1}) & tf(t_{2}, d_{1}) & tf(t_{3}, d_{1}) & tf(t_{4}, d_{1}) \\\\ tf(t_{1}, d_{2}) & tf(t_{2}, d_{2}) & tf(t_{3}, d_{2}) & tf(t_{4}, d_{2})  \\end{bmatrix} \\times \\begin{bmatrix} idf(t_{1}) & 0 & 0 & 0 \\\\ 0 & idf(t_{2}) & 0 & 0 \\\\ 0 & 0 & idf(t_{3}) & 0 \\\\ 0 & 0 & 0 & idf(t_{4}) \\end{bmatrix}$\n",
    "\n",
    "#####Let’s see now a concrete example of this multiplication:\n",
    "\n",
    "$M_{tf-idf} = M_{train} \\times M_{idf} = \\begin{bmatrix} 0 & 1 & 1 & 1 \\\\ 0 & 2 & 1 & 0 \\end{bmatrix} \\times \\begin{bmatrix} 0.6931 & 0 & 0 & 0 \\\\ 0 & -0.4055 & 0 & 0 \\\\ 0 & 0 & -0.4055 & 0 \\\\ 0 & 0 & 0 & 0.000 \\end{bmatrix} = \\begin{bmatrix} 0 & -0.4055 & -0.4055 & 0 \\\\ 0 & -0.8109 & -0.4055 & 0 \\end{bmatrix}$\n",
    "\n",
    "- finally, we can apply our L2 normalization process to the $M_{tf-idf}$ matrix. \n",
    "\n",
    "- note that this normalization is “row-wise” because we’re going to handle each row of the matrix as a separated vector to be normalized, and not the matrix as a whole:\n",
    "\n",
    "$M_{tf-idf} = \\frac{M_{tf-idf}}{\\|M_{tf-idf} \\|_{2}} = \\begin{bmatrix} 0 & -0.7071 & -0.7071 & 0 \\\\ 0 & -0.8944 & -0.4472 & 0 \\end{bmatrix}$\n",
    "\n",
    "- and that is the normalized tf-idf weight of our testing document set, which is actually a collection of unit vectors. \n",
    "\n",
    "- if we take the L2-norm of each row of the matrix, we’ll see that they all have a L2-norm of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Now in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sklearn routine produces the vocabulary in a different order to what was used above\n",
    "- The first step is to create our training and testing document set and computing the term frequency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {u'blue': 0, u'sun': 3, u'bright': 1, u'sky': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_set = [\"The sky is blue.\", \"The sun is bright.\"]\n",
    "test_set = [\"The sun in the sky is bright.\", \"We can see the shining sun, the bright sun.\"]\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "mcv = count_vectorizer.fit_transform(train_set)\n",
    "print \"Vocabulary:\", count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "freq_term_matrix = count_vectorizer.transform(test_set)\n",
    "print freq_term_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have the frequency term matrix (called \"freq_term_matrix\"), we can instantiate the tf-idf Transformer\n",
    "\n",
    "- The tf-idf transformer is going to be responsible for calculating the tf-idf weights for our term frequency matrix\n",
    "\n",
    "- Note that we’ve specified the norm as L2. This is optional (actually the default is L2-norm)\n",
    "\n",
    "- Also note that we can see the calculated idf weight by accessing the internal attribute called idf_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: [ 2.09861229  1.          1.40546511  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(norm=\"l2\", use_idf=True)\n",
    "tfidf.fit(freq_term_matrix)\n",
    "\n",
    "print \"IDF:\", tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#help(TfidfTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that the fit() method has calculated the idf for the matrix, let’s transform the freq_term_matrix to the tf-idf weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.50154891  0.70490949  0.50154891]\n",
      " [ 0.          0.4472136   0.          0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_matrix = tfidf.transform(freq_term_matrix)\n",
    "print tf_idf_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note that scikit-learn provides a function to do the vectorizing and tf-idf matrix calculation all in one, you can use TfidfVectorizer. \n",
    "\n",
    "- However, we must pass in a dictionary unless we want the dictionary to be learned from the documents we are analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.50154891  0.70490949  0.50154891]\n",
      " [ 0.          0.4472136   0.          0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_compare = TfidfVectorizer(vocabulary=count_vectorizer.vocabulary_).fit_transform(test_set)\n",
    "print tfidf_compare.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####The difference between this and the hand calculated result is due to the fact that sklearn appears not to allow zeros in the $M_{idf}$ matrix\n",
    "#####The bottom row of the final matrix is correct (allowing for the different order of the vocabulary)\n",
    "#####The top row has additional term which alters the tf-idf weights when normalization occurs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Cosine Similarity\n",
    "=====\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Dot Products\n",
    "\n",
    "- A dot product is the multiplication of 2 vectors\n",
    "\n",
    "- let's now consider the dot product from a geometric point of view:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Assets/Dot_Product.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The dot product of vector $A$ times vector $B$ is defined as:\n",
    "\n",
    "$A.B=||A||\\text{  }||B||\\text{  }cos\\theta$\n",
    "\n",
    "- the length of vector A times the length of vector B times the angle between them\n",
    "\n",
    "#####So, what happens when the angle between the two vectors is 90 degrees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Assets/orthogonal.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####*Q: What is the cosine of ninety degrees?*\n",
    "\n",
    "- When the angle between the two vectors is 90 degrees, then the term $cos{\\theta}$ will be zero and the resulting dot product will also be zero \n",
    "\n",
    "- note that although we are using 2D examples we can also calculate angles and similarity between vectors in higher dimensional spaces\n",
    "\n",
    "- we cannot visualize or imagine what the angle between two vectors with twelve dimensions is, for example, but we can calculate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Cosine Similarity (of documents in our vector space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the cosine similarity between two vectors (or two documents in the Vector Space) is a measure that calculates the cosine of the angle between them\n",
    "\n",
    "- This metric is a measurement of orientation and not magnitude\n",
    "\n",
    "- it can be seen as a comparison between documents on a normalized space\n",
    "\n",
    "- to calculate the cosine similarity is to solve for $cos{\\theta}$:\n",
    "\n",
    "$similarity = cos(\\theta) = \\frac{A.B}{||A||\\text{  }||B||} = \\frac{\\sum^{n}_{i=1}A_{i}\\times B_{i}}{\\sqrt{\\sum^{n}_{i=1}(A_{i})^2}\\times\\sqrt{\\sum^{n}_{i=1}(B_{i})^{2}}}$\n",
    "\n",
    "- Cosine Similarity will generate a metric that says how related two documents are by looking at the angle between them in vector space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Assets/cosine_similarity.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- magnitude - meaning the term frequency is largely ignored\n",
    "\n",
    "- so even if we have 2 vectors where one has a magnitude far exceeding the other, they could still have an small angle between them\n",
    "\n",
    "- this is the central point for the use of Cosine Similarity, the measurement tends to ignore the higher term count on documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Assets/vector_space.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = (\n",
    "\"The sky is blue\",\n",
    "\"The sun is bright\",\n",
    "\"The sun in the sky is bright\",\n",
    "\"We can see the shining sun, the bright sun\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.78528828  0.          0.          0.6191303   0.        ]\n",
      " [ 0.          0.70710678  0.          0.          0.70710678]\n",
      " [ 0.          0.53256952  0.          0.65782931  0.53256952]\n",
      " [ 0.          0.36626037  0.57381765  0.          0.73252075]]\n",
      "(4, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = \"english\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print tfidf_matrix.todense()\n",
    "print tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Now we have the TF-IDF matrix (tfidf_matrix) for each document (the number of rows of the matrix) with 4 tf-idf terms (the number of terms used in the index vocabulary), we can calculate the Cosine Similarity between the first document (“The sky is blue”) with each of the other documents of the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n"
     ]
    }
   ],
   "source": [
    "print tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.40728206  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix)\n",
    "print cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.40728206  0.        ]\n",
      " [ 0.          1.          0.75316704  0.77695558]\n",
      " [ 0.40728206  0.75316704  1.          0.58517734]\n",
      " [ 0.          0.77695558  0.58517734  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the tfidf_matrix[0:1] is the Scipy operation to get the first row of the sparse matrix and the resulting array is the Cosine Similarity between the first document with all documents in the set. \n",
    "\n",
    "- Note that the first value of the array is 1.0 because it is the Cosine Similarity between the first document with itself. \n",
    "\n",
    "- Also note that due to the presence of similar words on the third document (“The sun in the sky is bright”), it achieved a better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.407282057783\n",
      "65.9657881095\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print cos_sim[0][2]\n",
    "angle_in_radians = math.acos(cos_sim[0][2])\n",
    "print math.degrees(angle_in_radians)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
