{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "#This is recommended when using gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Latent Dirichlet Allocation (LDA)\n",
    "=====\n",
    "***\n",
    "\n",
    "(Adapted from http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Automatically discover **topics** within a set of **documents**.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the following sentences:\n",
    "\n",
    "```\n",
    "1. I like to eat broccoli and bananas.\n",
    "2. I ate a banana and spinach smoothie for breakfast.\n",
    "3. Chinchillas and kittens are cute.\n",
    "4. My sister adopted a kitten yesterday.\n",
    "5. Look at this cute hamster munching on a piece of broccoli.\n",
    "```\n",
    "\n",
    "For example, suppose you want to find the top two topics in the above sentenes. LDA might produce a result like:\n",
    "\n",
    "```\n",
    "Sentences 1 and 2: 100% Topic A\n",
    "Sentences 3 and 4: 100% Topic B\n",
    "Sentence 5: 60% Topic A, 40% Topic B\n",
    "Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, … (at which point, you could interpret topic A to be about food)\n",
    "Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, … (at which point, you could interpret topic B to be about cute animals)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The gensim tutorial](https://radimrehurek.com/gensim/tut1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Wikipedia, Latent Dirichlet Allocation\n",
    "\n",
    "1. Tell the algorithm how many topics you think there are\n",
    " - intuitively\n",
    " - statistically\n",
    "\n",
    "2. Assign every word to a topic in a semi-random manner (a dirichlet distribution)\n",
    " - a word can appear in more than one topic\n",
    "\n",
    "3. Iterate: Loop through every word in each topic and update it's topic assignemnt, according to:\n",
    "\n",
    "a. how prevalent is a word across topics, \n",
    "\n",
    "b. how prevalent are topics in the document\n",
    "\n",
    "Looking at each topic what proportion of the topic is down to each word. Certain words will favor certain topics.\n",
    "\n",
    "Looking at each document how prevalent are the topics. Divide up the document into the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I eat fish and vegetables\n",
    "- Fish are pets\n",
    "- My kitten eats fish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ask for 2 topics:\n",
    "\n",
    "Topic A: eat fish, eats fish, vegetables\n",
    "    \n",
    "Topic B: Fish, pets, kitten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Infer the content spread of each sentence by word count\n",
    "\n",
    "- Sentence 1: 100% Topic A\n",
    "\n",
    "- Sentence 2: 100% Topic B\n",
    "\n",
    "- Sentence 3: 33% Topic B and 66% Topic A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Can derive the porportions that each word constitutes in given topics\n",
    "\n",
    "- Topic A might comprise words in the following proportions: 40% eat, 40% fish, 20%vegetables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Documents represented as strings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine computer interface for lab abc computer applications\",\n",
    "              \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tokenize the documents, remove stop words and words that only appear once in the corpus\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Firstly let's tokenize the documents, and remove stop words using a 'toy' stop-word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['a', 'and', 'for', 'of', 'to', 'in', 'the'])\n"
     ]
    }
   ],
   "source": [
    "stop_list = set(['for', 'a', 'of', 'the', 'and', 'to', 'in'])\n",
    "print stop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> tip: Consider using https://pypi.python.org/pypi/stop-words for more robust stop-word analysis\n",
    "\n",
    "```python\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "en_stop = get_stop_words('en')\n",
    "stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents_without_stops = []\n",
    "for docs in documents:\n",
    "    t = [word for word in docs.lower().split() if word not in stop_list]\n",
    "    documents_without_stops.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'computer', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "print documents_without_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for text in documents_without_stops:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for text in documents_without_stops:\n",
    "    t = [token for token in text if frequency[token] > 1]\n",
    "    texts.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'computer', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now convert documents to vectors, this is a bag-of-words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dictionary is  Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print \"dictionary is \", dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are 12 distinct words, so each document will be represented by a 12-D vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It is possible also to display the token id's that the words have been mapped to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary to token:\n",
      "{u'computer': 1,\n",
      " u'eps': 8,\n",
      " u'graph': 10,\n",
      " u'human': 2,\n",
      " u'interface': 0,\n",
      " u'minors': 11,\n",
      " u'response': 3,\n",
      " u'survey': 5,\n",
      " u'system': 6,\n",
      " u'time': 4,\n",
      " u'trees': 9,\n",
      " u'user': 7}\n"
     ]
    }
   ],
   "source": [
    "print \"Dictionary to token:\" \n",
    "pprint(dictionary.token2id) # pprint is short for \"pretty print\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The function doc2bow is like the python CountVectorizer. It counts frequency of occurrence of words in each document and returns a spares matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'computer', 'interface', 'computer']\n",
      "[(0, 1), (1, 2), (2, 1)]\n",
      "['survey', 'user', 'computer', 'system', 'response', 'time']\n",
      "[(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "['eps', 'user', 'interface', 'system']\n",
      "[(0, 1), (6, 1), (7, 1), (8, 1)]\n",
      "['system', 'human', 'system', 'eps']\n",
      "[(2, 1), (6, 2), (8, 1)]\n",
      "['user', 'response', 'time']\n",
      "[(3, 1), (4, 1), (7, 1)]\n",
      "['trees']\n",
      "[(9, 1)]\n",
      "['graph', 'trees']\n",
      "[(9, 1), (10, 1)]\n",
      "['graph', 'minors', 'trees']\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "['graph', 'minors', 'survey']\n",
      "[(5, 1), (10, 1), (11, 1)]\n",
      "\n",
      "\n",
      "\n",
      "[(0, 1), (1, 2), (2, 1)]\n",
      "[(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(0, 1), (6, 1), (7, 1), (8, 1)]\n",
      "[(2, 1), (6, 2), (8, 1)]\n",
      "[(3, 1), (4, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(5, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    print text\n",
    "    print dictionary.doc2bow(text)\n",
    "    \n",
    "print \"\\n\\n\"\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#remember there are 12 tokens, and you need the dictionary to token information to work out the coding\n",
    "\n",
    "for c in corpus:\n",
    "    print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The LDA model converts the bag-of-words representation into a topic-space of lower dimensionality\n",
    "##### LDA's topics are probability distributions over words\n",
    "##### The distributions are inferred automatically from the corpus\n",
    "##### Documents are then interpreted as a mixture of these topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=2, passes = 10, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "topic 0:\n",
      "(0, u'0.171*graph + 0.159*computer')\n",
      "\n",
      "\n",
      "topic 1:\n",
      "(1, u'0.203*system + 0.157*user')\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(lda_model.print_topics(num_topics = 2, num_words = 2)):\n",
    "    print \"\\n\\ntopic {:d}:\\n\".format(i), topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Classification to topic, with accompanying probability\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_doc = 'the grass is greener'\n",
    "new_doc1 = 'Human Computer Interaction'\n",
    "new_doc2 = 'Graphs are excellent data structures and are related to trees'\n",
    "\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "new_vec1 = dictionary.doc2bow(new_doc1.lower().split())\n",
    "new_vec2 = dictionary.doc2bow(new_doc2.lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divides the documents up into topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5), (1, 0.5)]\n",
      "[(0, 0.20654388799196441), (1, 0.79345611200803567)]\n",
      "[(0, 0.74678800888043906), (1, 0.25321199111956094)]\n"
     ]
    }
   ],
   "source": [
    "print lda_model[new_vec]\n",
    "print lda_model[new_vec1]\n",
    "print lda_model[new_vec2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review:\n",
    "\n",
    "*Why use LDA?*\n",
    "\n",
    "**Automatically discover *topics* within a set of *documents***\n",
    "*LDA represents documents as mixtures of topics that spit out words with certain probabilities.*\n",
    "\n",
    "### Additional Resources\n",
    "* [Gensim tutorial](https://radimrehurek.com/gensim/tut1.html)\n",
    "* [Cosine Similarity Exercise](http://blog.christianperone.com/?p=1589)\n",
    "* [Tutorial on using NLTK](http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk)\n",
    "* [Lecture notes on NLP](http://cs.nyu.edu/courses/spring04/G22.2591-001/lecture3.html)\n",
    "* [Information about the 20 newsgroups dataset](http://qwone.com/~jason/20Newsgroups/)\n",
    "* [http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/] (Useful intro to LDA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
